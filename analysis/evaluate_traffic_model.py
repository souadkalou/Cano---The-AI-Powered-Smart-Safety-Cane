"""
===============================================================================
Evaluate Traffic Light Classifier & Baselines
===============================================================================

This script evaluates our main TrafficLightCNN model and compares it against
two baselines:

    1) Random baseline
    2) Logistic Regression baseline using only brightness as a feature

It produces:
    • Confusion matrix (PNG)
    • ROC curve (PNG)
    • Model comparison table (CSV + console print)

HOW THIS FITS THE PROJECT REQUIREMENTS
--------------------------------------
The course checklist asks for:

    • Classification metrics:
        - Accuracy
        - Precision
        - Recall
        - F1-Score
        - ROC Curve + AUC
    • A model comparison table:
        - Multiple models compared on the same metrics
    • Proper use of train/val/test or equivalent use of held-out data

This script satisfies those by:
    • Evaluating the trained TrafficLightCNN on a test split
    • Training a logistic regression baseline
    • Creating a random baseline
    • Aggregating all metrics in a single comparison DataFrame and CSV

Run from project root:

    python analysis/evaluate_traffic_model.py

Dependencies required:
    - data/traffic_lights_metadata.csv
        (generated by analysis/eda_traffic_lights.py)
    - models/traffic_light_classifier/best_traffic_light.pt
        (trained by src/training/train_traffic_light.py)
    - src/models/traffic_light_model.py with TrafficLightCNN defined
"""

from pathlib import Path          # For safe and clear path handling
import sys                        # To modify Python path for imports
import numpy as np                # For numerical operations on arrays
import pandas as pd               # For tabular data (metadata + comparison table)
import matplotlib.pyplot as plt   # For plotting confusion matrix & ROC
from PIL import Image             # For loading images from disk

# Scikit-learn tools for baselines and evaluation metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
)
from sklearn.model_selection import train_test_split

# PyTorch imports for loading and evaluating our CNN
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# -------------------------------------------------------
# Paths & Python path fix
# -------------------------------------------------------

# ROOT_DIR points to the project root (…/cano-smart-cane)
ROOT_DIR = Path(__file__).resolve().parents[1]

# We make sure the project root is on sys.path so that:
#   from src.models.traffic_light_model import TrafficLightCNN
# works even when running from the project root.
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

# File generated by EDA step: includes brightness, RGB means, etc.
METADATA_CSV = ROOT_DIR / "data" / "traffic_lights_metadata.csv"

# Trained CNN weights saved by train_traffic_light.py
MODEL_WEIGHTS = ROOT_DIR / "models" / "traffic_light_classifier" / "best_traffic_light.pt"

# Folder for saving evaluation plots (confusion matrix + ROC curve)
PLOTS_DIR = ROOT_DIR / "analysis" / "plots"
PLOTS_DIR.mkdir(parents=True, exist_ok=True)

# Import our custom CNN class now that ROOT_DIR is on sys.path
from src.models.traffic_light_model import TrafficLightCNN  # type: ignore

# Decide whether to run on GPU (if available) or CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# -------------------------------------------------------
# Dataset class for CNN evaluation
# -------------------------------------------------------
class TrafficLightsDataset(Dataset):
    """
    Small Dataset class for wrapping the metadata CSV rows into
    (image_tensor, label) pairs for PyTorch evaluation.

    We reuse the processed metadata from EDA instead of scanning folders again.
    """

    def __init__(self, df: pd.DataFrame, transform=None):
        """
        Parameters
        ----------
        df : pd.DataFrame
            A subset of the metadata DataFrame with at least:
                - 'filepath' (path to image)
                - 'label' ("red" or "green")

        transform : callable or None
            Optional transform to apply to each loaded image
            (resize, normalization, etc.).
        """
        self.df = df.reset_index(drop=True)  # clean index for __getitem__
        self.transform = transform

        # Map string labels to numeric class IDs
        self.label_map = {"red": 0, "green": 1}

    def __len__(self):
        """Return total number of samples in this dataset."""
        return len(self.df)

    def __getitem__(self, idx):
        """
        Load one sample by index and return (image_tensor, label_id).

        Steps:
            1. Read the file path from the DataFrame.
            2. Open image and convert to RGB.
            3. Apply transforms if provided.
            4. Convert label string to integer ID.
        """
        row = self.df.iloc[idx]
        img = Image.open(row["filepath"]).convert("RGB")

        if self.transform:
            img = self.transform(img)

        label = self.label_map[row["label"]]
        return img, label


# -------------------------------------------------------
# Helper functions
# -------------------------------------------------------
def eval_torch_model(model, loader):
    """
    Evaluate a PyTorch classification model on the given DataLoader.

    Returns a dictionary with:
        - accuracy
        - precision
        - recall
        - f1
        - roc_auc
        - y_true (numpy array of ground-truth labels)
        - y_pred (binary predicted labels)
        - y_prob (predicted probability of 'green')

    Why prob of 'green'?
        We treat class '1' as "green", so we pull softmax[:, 1]
        to compute ROC AUC and thresholded predictions.
    """

    all_y = []      # List to store true labels
    all_probs = []  # List to store predicted probabilities

    model.eval()  # Set the model to evaluation mode (no dropout, no grad)
    with torch.no_grad():  # Disable gradient computation for speed
        for imgs, labels in loader:
            imgs = imgs.to(device)
            labels = labels.to(device)

            # Forward pass: raw logits
            outputs = model(imgs)

            # Convert logits to probabilities via softmax
            probs = torch.softmax(outputs, dim=1)[:, 1]  # probability of class 1 (GREEN)

            # Aggregate results for metrics
            all_probs.extend(probs.cpu().numpy())
            all_y.extend(labels.cpu().numpy())

    # Turn lists into numpy arrays
    y_true = np.array(all_y)
    y_prob = np.array(all_probs)

    # Threshold probabilities at 0.5 to obtain hard class predictions
    y_pred = (y_prob >= 0.5).astype(int)

    # Build the metrics dictionary
    metrics = {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred),
        "recall": recall_score(y_true, y_pred),
        "f1": f1_score(y_true, y_pred),
        "roc_auc": roc_auc_score(y_true, y_prob),
        "y_true": y_true,
        "y_pred": y_pred,
        "y_prob": y_prob,
    }
    return metrics


def plot_confusion_matrix(y_true, y_pred, classes, filename):
    """
    Draw and save a confusion matrix plot.

    Parameters
    ----------
    y_true : array-like
        True labels.

    y_pred : array-like
        Predicted labels.

    classes : list
        List of class names in order (e.g. ["red", "green"]).

    filename : str
        The filename to save in PLOTS_DIR.
    """
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(4, 4))
    plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    # Add numeric labels to each cell
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(
                j,
                i,
                cm[i, j],
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black",
            )

    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.tight_layout()

    out = PLOTS_DIR / filename
    plt.savefig(out)
    plt.close()
    print(f"Saved {out}")


def plot_roc(y_true, y_prob, filename):
    """
    Plot and save the ROC curve for a binary classifier.

    Parameters
    ----------
    y_true : array-like
        Ground-truth binary labels (0 or 1).

    y_prob : array-like
        Predicted probabilities for the positive class (1 = "green").

    filename : str
        Output filename under PLOTS_DIR.
    """
    from sklearn.metrics import roc_curve

    # Compute false positive rate (FPR) and true positive rate (TPR)
    fpr, tpr, _ = roc_curve(y_true, y_prob)

    plt.figure(figsize=(5, 4))
    plt.plot(fpr, tpr, label="TrafficLightCNN")
    plt.plot([0, 1], [0, 1], "k--", label="Random")  # diagonal = random classifier
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.tight_layout()

    out = PLOTS_DIR / filename
    plt.savefig(out)
    plt.close()
    print(f"Saved {out}")


# -------------------------------------------------------
# Main evaluation pipeline
# -------------------------------------------------------
def main():
    """
    Main function orchestrating:

        1. Loading metadata
        2. Splitting into train/val/test subsets
        3. Evaluating the trained TrafficLightCNN
        4. Evaluating a logistic regression baseline
        5. Evaluating a random baseline
        6. Generating plots (confusion matrix, ROC)
        7. Saving and printing a model comparison table
    """

    # Ensure the metadata CSV exists before proceeding
    if not METADATA_CSV.exists():
        raise FileNotFoundError(
            f"Metadata CSV not found at {METADATA_CSV}. "
            "Run eda_traffic_lights.py first."
        )

    # Load metadata produced by EDA script
    df = pd.read_csv(METADATA_CSV)

    # ------------------------------------------------------------------
    # 1) Shuffle and create train/val/test splits
    #
    # We manually create splits rather than relying on the training script,
    # to ensure consistent evaluation for all models.
    # ------------------------------------------------------------------
    df_shuffled = df.sample(frac=1, random_state=42)
    n = len(df_shuffled)

    # 70% train, 15% val, 15% test (we only evaluate on test below)
    train_df = df_shuffled.iloc[: int(0.7 * n)]
    val_df = df_shuffled.iloc[int(0.7 * n) : int(0.85 * n)]
    test_df = df_shuffled.iloc[int(0.85 * n) :]

    print(f"Train size: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

    # ------------------------------------------------------------------
    # 2) Prepare PyTorch datasets/transforms for the CNN evaluation
    # ------------------------------------------------------------------
    train_transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),             # same size as training
            transforms.RandomHorizontalFlip(),         # simple augmentation
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],           # standard ImageNet values
                std=[0.229, 0.224, 0.225],
            ),
        ]
    )
    test_transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
        ]
    )

    # We only need test_ds/test_loader for evaluation here
    test_ds = TrafficLightsDataset(test_df, transform=test_transform)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

    # ------------------------------------------------------------------
    # 3) Load the trained TrafficLightCNN model and evaluate it
    # ------------------------------------------------------------------
    model = TrafficLightCNN(num_classes=2).to(device)

    # Load the best weight file trained earlier
    state = torch.load(MODEL_WEIGHTS, map_location=device)
    model.load_state_dict(state)

    # Evaluate on test set
    cnn_metrics = eval_torch_model(model, test_loader)

    print("\n=== TrafficLightCNN Metrics (Test Set) ===")
    for k in ["accuracy", "precision", "recall", "f1", "roc_auc"]:
        print(f"{k}: {cnn_metrics[k]:.4f}")

    # Save confusion matrix and ROC plots
    plot_confusion_matrix(
        cnn_metrics["y_true"],
        cnn_metrics["y_pred"],
        classes=["red", "green"],
        filename="cm_traffic_light_cnn.png",
    )
    plot_roc(
        cnn_metrics["y_true"],
        cnn_metrics["y_prob"],
        filename="roc_traffic_light_cnn.png",
    )

    # ------------------------------------------------------------------
    # 4) Baseline model: Logistic Regression using only brightness feature
    #
    # This baseline answers: “If we only look at brightness, how well can
    # we separate red vs green traffic lights?”
    # ------------------------------------------------------------------
    X = df[["brightness"]].values
    y = df["label"].map({"red": 0, "green": 1}).values

    # 80/20 split for baseline evaluation
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    # Train logistic regression baseline
    logreg = LogisticRegression()
    logreg.fit(X_train, y_train)

    # Predicted probabilities and labels
    y_prob_lr = logreg.predict_proba(X_test)[:, 1]   # prob of GREEN
    y_pred_lr = (y_prob_lr >= 0.5).astype(int)

    baseline_metrics = {
        "accuracy": accuracy_score(y_test, y_pred_lr),
        "precision": precision_score(y_test, y_pred_lr),
        "recall": recall_score(y_test, y_pred_lr),
        "f1": f1_score(y_test, y_pred_lr),
        "roc_auc": roc_auc_score(y_test, y_prob_lr),
    }

    print("\n=== Baseline Logistic Regression (brightness only) ===")
    for k, v in baseline_metrics.items():
        print(f"{k}: {v:.4f}")

    # ------------------------------------------------------------------
    # 5) Random baseline (for sanity check)
    #
    # We generate random predictions to show how a non-informative classifier
    # would perform compared to our CNN and logistic regression.
    # ------------------------------------------------------------------
    rng = np.random.RandomState(0)
    y_pred_rand = rng.randint(0, 2, size=len(y_test))   # random 0/1 labels
    y_prob_rand = rng.rand(len(y_test))                 # random probabilities

    random_metrics = {
        "accuracy": accuracy_score(y_test, y_pred_rand),
        "precision": precision_score(y_test, y_pred_rand),
        "recall": recall_score(y_test, y_pred_rand),
        "f1": f1_score(y_test, y_pred_rand),
        "roc_auc": roc_auc_score(y_test, y_prob_rand),
    }

    # ------------------------------------------------------------------
    # 6) Build and save model comparison table
    # ------------------------------------------------------------------
    comparison = pd.DataFrame(
        [
            {
                "Model": "Random baseline",
                "Accuracy": random_metrics["accuracy"],
                "Precision": random_metrics["precision"],
                "Recall": random_metrics["recall"],
                "F1": random_metrics["f1"],
                "ROC_AUC": random_metrics["roc_auc"],
                "Notes": "Random guesses (no learning)",
            },
            {
                "Model": "Brightness Logistic Regression",
                "Accuracy": baseline_metrics["accuracy"],
                "Precision": baseline_metrics["precision"],
                "Recall": baseline_metrics["recall"],
                "F1": baseline_metrics["f1"],
                "ROC_AUC": baseline_metrics["roc_auc"],
                "Notes": "Single-feature baseline (brightness only)",
            },
            {
                "Model": "TrafficLightCNN",
                "Accuracy": cnn_metrics["accuracy"],
                "Precision": cnn_metrics["precision"],
                "Recall": cnn_metrics["recall"],
                "F1": cnn_metrics["f1"],
                "ROC_AUC": cnn_metrics["roc_auc"],
                "Notes": "Deep CNN trained on cropped red/green lights",
            },
        ]
    )

    print("\n=== Model Comparison Table ===")
    print(comparison.to_string(index=False))

    out_csv = ROOT_DIR / "analysis" / "traffic_light_model_comparison.csv"
    comparison.to_csv(out_csv, index=False)
    print(f"\nSaved comparison table to {out_csv}")


# Only run main() if this script is executed directly
if __name__ == "__main__":
    main()